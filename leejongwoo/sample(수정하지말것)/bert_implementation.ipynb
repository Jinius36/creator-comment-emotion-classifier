{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2025 2학기 기계학습기초 '사용자 댓글 기반 소셜 반응 시각화 모델' 팀\n",
        "\n",
        "이 문서에서는 PyTorch를 사용하여 BERT 모델을 구현한 과정을 다룹니다.\n",
        "\n",
        "## 1. BERT 모델 구현을 위한 데이터 전처리 파일 가져오기\n",
        "\n",
        "#### 1.1 구글 드라이브 접근\n",
        "[Hugging Face의 dair-ai/emotion 데이터셋](https://huggingface.co/datasets/dair-ai/emotion)을 이용해 팀원이 만든 model과 vocab을 구글 드라이브에 저장한 뒤 마운트하여 사용할 수 있습니다.\n",
        "\n",
        "우선 아래 코드를 실행시켜 허용을 눌러주면, 왼쪽 '파일'에 내 구글 드라이브가 마운트 됩니다.\n",
        "이를 확인해보세요."
      ],
      "metadata": {
        "id": "s9bb3-uqbaN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3hPhbSVgmg",
        "outputId": "ef61cfe2-5bae-4e53-fd9b-7ec8b8838ef5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이후 [BERT 구현에 사용할 전처리된 데이터 model, vocab](https://github.com/Jinius36/creator-comment-emotion-classifier/tree/main/bert-implementation/data-preprocessing)에서 파일을 다운로드 하여 내 구글 드라이브에 저장하고, 저장한 장소로 아래 디렉토리를 수정해주세요."
      ],
      "metadata": {
        "id": "2zw2fQUSViWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jJTsUIiMuHqS"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/bert\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 잘 마운트 되었는지 아래 코드를 실행시켜 확인해봅시다."
      ],
      "metadata": {
        "id": "up0vPFbZXA2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt5_RycqXDmL",
        "outputId": "da24a64f-8613-4f7e-bccc-9ee1580b0690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mini_emotion_tokenizer_11k.model\n",
            "mini_emotion_tokenizer_11k.vocab\n",
            "mini_emotion_tokenizer_7k.model\n",
            "mini_emotion_tokenizer_7k.vocab\n",
            ".ipynb_checkpoints\n",
            "emotion_bert_pretrain_00.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 마운트한 Vocab 확인하기\n",
        "가져온 vocab을 로딩해봅시다."
      ],
      "metadata": {
        "id": "_hU3LfhkXi31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "# vocab loading\n",
        "vocab_file = f\"{data_dir}/mini_emotion_tokenizer_7k.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eRoWX0oYOCk",
        "outputId": "bce7d96f-3b91-4212-c428-73c0a460da9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. BERT 모델 개요\n",
        "\n",
        "구글이 [2018년 발표한 BERT 논문](https://arxiv.org/pdf/1810.04805)에서 제안된 모델의 구조는 이렇습니다.\n",
        "<p>\n",
        "<img src=\"https://raw.githubusercontent.com/Jinius36/creator-comment-emotion-classifier/main/bert-implementation/colab/bert-outline-diagram.png\" width = \"30%\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "구조를 확인해보면, 표준 Transformer 모델에서 EncoderLayer 부분은 동일하게 가져오되 Encoder 부분에서 token, position 외에 'segment embedding'을 추가해야 함을 확인할 수 있습니다.   \n",
        "\n",
        "또한, BERT 모델의 파라미터는 Base 모델과 Large 모델로 다음과 같음을 확인하였습니다.   \n",
        "| **파라미터 종류**         | **BERT-Base** | **BERT-Large** |\n",
        "|---------------------------|---------------|----------------|\n",
        "| Layer 수                  | 12            | 24             |\n",
        "| Embedding 차원            | 768           | 1024           |\n",
        "| Self-Attention Head 수    | 12            | 16             |\n",
        "| Feed-Forward 차원         | 3072          | 4096           |\n",
        "| 총 파라미터 수            | 약 1억 1천만  | 약 3억 4천만   |\n",
        "| 최대 입력 토큰 수         | 512           | 512            |\n",
        "| Vocabulary 크기           | 30522         | 30522          |"
      ],
      "metadata": {
        "id": "Xhl326orbx7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. config 제작\n",
        "이번 BERT 모델 구현은 다음과 같이 설정하여 진행해보겠습니다.   \n",
        "| **파라미터 종류**           | **나의 BERT 구성** |\n",
        "| --------------------- | -------------- |\n",
        "| Layer 수               | 6              |\n",
        "| Embedding 차원          | 256            |\n",
        "| Self-Attention Head 수 | 4              |\n",
        "| Feed-Forward 차원       | 1024           |\n",
        "| 총 입력 문장 수 (훈련셋)       | 436,809        |\n",
        "| 최대 입력 토큰 수            | 256            |\n",
        "| Vocabulary 크기         | 7[링크 텍스트](https://),000         |\n",
        "| Segment Type 수        | 2              |\n",
        "| Dropout               | 0.1            |\n",
        "| LayerNorm Epsilon     | 1e-12          |\n",
        "| Pad Token Index       | 0              |\n",
        "\n",
        "이는 우리 팀에서 구현하는 BERT 모델의 기본 모델이며,\n",
        "추후 성능 테스트를 진행할 때 config를 수정하여 모델을 수정해볼 수도 있도록 하였습니다."
      ],
      "metadata": {
        "id": "aOLbyE__nKHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# configuration을 읽어들이는 class\n",
        "class Config(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "metadata": {
        "id": "v8VdZTzicVyH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config({\n",
        "    \"n_enc_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_seg_type\": 2,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidn\": 256,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKFcCP48q2gv",
        "outputId": "c2073b21-1ab9-48a8-9278-e6d0b62c0c30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_enc_vocab': 7000, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 공통 함수\n",
        "표준 Transformer에서 공통으로 사용되는 함수와 클래스를 정의해놓았습니다.\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "heHpm_SHrVhE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY99m9lMRMy2"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 위치 인코딩 생성 함수 (Sinusoidal 방식)\n",
        "def create_sinusoidal_position_encoding(n_seq, d_hidn):\n",
        "    \"\"\"\n",
        "    Sinusoidal 방식의 위치 인코딩 테이블 생성\n",
        "\n",
        "    Args:\n",
        "        n_seq (int): 최대 시퀀스 길이\n",
        "        d_hidn (int): 임베딩 차원 수\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: [n_seq, d_hidn] 위치 인코딩 매트릭스\n",
        "    \"\"\"\n",
        "    def get_angle(pos, i):\n",
        "        return pos / np.power(10000, 2 * (i // 2) / d_hidn)\n",
        "\n",
        "    table = np.array([\n",
        "        [get_angle(pos, i) for i in range(d_hidn)]\n",
        "        for pos in range(n_seq)\n",
        "    ])\n",
        "    table[:, 0::2] = np.sin(table[:, 0::2])\n",
        "    table[:, 1::2] = np.cos(table[:, 1::2])\n",
        "    return table\n",
        "\n",
        "# 어텐션 마스크 생성 (패딩 토큰에 대한 마스크)\n",
        "def create_padding_mask(seq_q, seq_k, pad_idx):\n",
        "    \"\"\"\n",
        "    패딩 토큰에 대한 어텐션 마스크 생성\n",
        "\n",
        "    Args:\n",
        "        seq_q (Tensor): Query 시퀀스 [batch_size, seq_len_q]\n",
        "        seq_k (Tensor): Key 시퀀스 [batch_size, seq_len_k]\n",
        "        pad_idx (int): 패딩 토큰 인덱스\n",
        "\n",
        "    Returns:\n",
        "        Tensor: [batch_size, seq_len_q, seq_len_k] 마스크 텐서\n",
        "    \"\"\"\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    mask = seq_k.eq(pad_idx).unsqueeze(1).expand(batch_size, len_q, len_k)\n",
        "    return mask\n",
        "\n",
        "# 디코더용 어텐션 마스크 생성 (미래 토큰 가리기)\n",
        "def create_look_ahead_mask(seq):\n",
        "    \"\"\"\n",
        "    디코더에서 미래 토큰을 가리기 위한 마스크 생성\n",
        "\n",
        "    Args:\n",
        "        seq (Tensor): 시퀀스 텐서 [batch_size, seq_len]\n",
        "\n",
        "    Returns:\n",
        "        Tensor: [batch_size, seq_len, seq_len] 상삼각 행렬 마스크\n",
        "    \"\"\"\n",
        "    size = seq.size(1)\n",
        "    mask = torch.ones_like(seq).unsqueeze(-1).expand(-1, size, size)\n",
        "    return mask.triu(1)\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.scale = 1 / (config.d_head ** 0.5)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        어텐션 확률 계산 및 문맥 벡터 생성\n",
        "\n",
        "        Args:\n",
        "            Q, K, V (Tensor): Query, Key, Value 행렬\n",
        "            mask (Tensor): 어텐션 마스크 [batch, head, q_len, k_len]\n",
        "\n",
        "        Returns:\n",
        "            context: 문맥 벡터\n",
        "            attn: 어텐션 확률\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) * self.scale\n",
        "        scores.masked_fill_(mask, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.W_Q = nn.Linear(config.d_hidn, config.n_head * config.d_head)\n",
        "        self.W_K = nn.Linear(config.d_hidn, config.n_head * config.d_head)\n",
        "        self.W_V = nn.Linear(config.d_hidn, config.n_head * config.d_head)\n",
        "        self.scaled_attn = ScaledDotProductAttention(config)\n",
        "        self.linear = nn.Linear(config.n_head * config.d_head, config.d_hidn)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        어텐션 연산 수행 및 출력을 합치는 과정\n",
        "\n",
        "        Returns:\n",
        "            output: [batch, seq_len, d_hidn]\n",
        "            attn: [batch, head, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        B = Q.size(0)\n",
        "        q_s = self.W_Q(Q).view(B, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(B, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(B, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
        "        mask = mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "        context, attn = self.scaled_attn(q_s, k_s, v_s, mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(B, -1, self.config.n_head * self.config.d_head)\n",
        "        output = self.linear(context)\n",
        "        return self.dropout(output), attn\n",
        "\n",
        "# Position-wise Feed Forward Network (1x1 Conv 사용)\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(config.d_hidn, config.d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(config.d_ff, config.d_hidn, kernel_size=1)\n",
        "        self.activation = F.gelu\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Conv1D -> GELU -> Conv1D -> Dropout\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): [batch, seq_len, d_hidn]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: [batch, seq_len, d_hidn]\n",
        "        \"\"\"\n",
        "        x = self.conv1(x.transpose(1, 2))\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x).transpose(1, 2)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Encoder 구현\n",
        "이제 표준 Transformer Encoder를 수정하여 Encoder를 구현해보겠습니다.\n",
        "<p>\n",
        "<img src=\"https://raw.githubusercontent.com/Jinius36/creator-comment-emotion-classifier/main/bert-implementation/colab/Figure-2-in-BERT-paper.png\"/>\n",
        "</p>\n",
        "<caption>\n",
        "Figure 2. BERT의 입력 임베딩 구성 방식. 토큰 임베딩, 세그먼트 임베딩, 위치 임베딩의 합으로 구성됨.  \n",
        "출처: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. arXiv preprint arXiv:1810.04805.\n",
        "</caption>\n",
        "\n",
        "위 Figure 2에서 확인할 수 있듯 표준 Transformer Encoder에 'Segment Embeddings'를 추가하여 구현해야 합니다.   \n",
        "구현한 코드는 아래와 같습니다."
      ],
      "metadata": {
        "id": "J2gPuBsNt_8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Self-Attention 인코더 블록 정의\n",
        "class SelfAttentionEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    인코더 블록: Multi-Head Attention과 Feed-Forward Network를 포함하며,\n",
        "    각각의 출력에는 Residual Connection과 Layer Normalization이 적용됩니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # 다중 자기 주의 메커니즘\n",
        "        self.multi_head_attn = MultiHeadSelfAttention(config)\n",
        "        # 첫 번째 Layer Normalization\n",
        "        self.norm_after_attn = nn.LayerNorm(config.d_hidn, eps=config.layer_norm_epsilon)\n",
        "        # 위치 기반 FeedForward Network\n",
        "        self.feed_forward = PositionwiseFeedForward(config)\n",
        "        # 두 번째 Layer Normalization\n",
        "        self.norm_after_ffn = nn.LayerNorm(config.d_hidn, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, x, attn_mask):\n",
        "        # 자기 주의 연산 및 attention 확률 반환\n",
        "        attn_output, attn_weights = self.multi_head_attn(x, x, x, attn_mask)\n",
        "        # 첫 번째 잔차 연결 및 정규화\n",
        "        x = self.norm_after_attn(x + attn_output)\n",
        "        # Feed Forward 통과 후 두 번째 잔차 연결 및 정규화\n",
        "        ffn_output = self.feed_forward(x)\n",
        "        x = self.norm_after_ffn(x + ffn_output)\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "# 전체 Transformer 인코더 정의\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder 전체 구성:\n",
        "    임베딩(토큰, 위치, 세그먼트) 후 다수의 인코더 블록을 순차적으로 통과합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # 토큰 임베딩, 위치 임베딩, 세그먼트 임베딩 정의\n",
        "        self.token_embedding = nn.Embedding(config.n_enc_vocab, config.d_hidn)\n",
        "        self.position_embedding = nn.Embedding(config.n_enc_seq + 1, config.d_hidn)\n",
        "        self.segment_embedding = nn.Embedding(config.n_seg_type, config.d_hidn)\n",
        "\n",
        "        # 인코더 블록 여러 층 정의\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            SelfAttentionEncoderBlock(config) for _ in range(config.n_layer)\n",
        "        ])\n",
        "\n",
        "    def forward(self, token_ids, segment_ids):\n",
        "        # 위치 인덱스 생성 (패딩은 0으로 마스킹)\n",
        "        seq_length = token_ids.size(1)\n",
        "        device = token_ids.device\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).expand_as(token_ids) + 1\n",
        "        position_ids = position_ids.masked_fill(token_ids.eq(self.config.i_pad), 0)\n",
        "\n",
        "        # 세 가지 임베딩의 합산\n",
        "        x = (\n",
        "            self.token_embedding(token_ids)\n",
        "            + self.position_embedding(position_ids)\n",
        "            + self.segment_embedding(segment_ids)\n",
        "        )\n",
        "\n",
        "        # 패딩 마스크 생성\n",
        "        attn_mask = create_padding_mask(token_ids, token_ids, self.config.i_pad)\n",
        "\n",
        "        all_attention_weights = []\n",
        "        for block in self.encoder_blocks:\n",
        "            x, attn_weights = block(x, attn_mask)\n",
        "            all_attention_weights.append(attn_weights)\n",
        "\n",
        "        return x, all_attention_weights"
      ],
      "metadata": {
        "id": "XOJyrC8VyJtI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. BERT 구현\n",
        "이제 BERT 모델을 구현해보겠습니다.\n",
        "\n",
        "BERTModel 클래스의 각 함수를 설명해보면 다음과 같습니다.   \n",
        "- init에서 위에서 만든 ransformerEncoder를 실행하여 전체 문장의 Contextual Embedding을 생성합니다.   \n",
        "- forward에서 Encoder의 출력 중 첫 번째 위치의 토큰([CLS]) 벡터를 cls_output으로 추출하고,    \n",
        "추출한 [CLS] 벡터에 대해 선형 변환과 tanh 활성화를 적용하여 문장 표현을 완성합니다.   \n",
        "- save 함수는 학습 epoch, loss, 모델 가중치를 지정된 경로에 저장하며,   \n",
        " load 함수는 저장된 모델 상태를 로드해 학습이 중단된 epoch과 loss를 반환합니다."
      ],
      "metadata": {
        "id": "tqfiZNnKzRVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT 모델 정의:\n",
        "    - Transformer Encoder를 기반으로 [CLS] 벡터를 추출하여 활용합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # 인코더 모듈 (Transformer 구조 기반)\n",
        "        self.encoder = TransformerEncoder(self.config)\n",
        "\n",
        "        # [CLS] 토큰 출력을 위한 추가 선형 변환 및 활성화 함수\n",
        "        self.linear_cls = nn.Linear(config.d_hidn, config.d_hidn)\n",
        "        self.activation_cls = torch.tanh\n",
        "\n",
        "    def forward(self, input_ids, segment_ids):\n",
        "        \"\"\"\n",
        "        입력:\n",
        "          - input_ids: 토큰 ID 시퀀스 (batch_size, n_seq)\n",
        "          - segment_ids: 세그먼트 정보 (batch_size, n_seq)\n",
        "\n",
        "        출력:\n",
        "          - 전체 인코더 출력 (contextual embeddings)\n",
        "          - [CLS] 토큰 출력 (문장 표현)\n",
        "          - 각 층별 attention 가중치 리스트\n",
        "        \"\"\"\n",
        "        # 인코더를 통과시켜 contextual embedding 획득\n",
        "        encoder_output, attention_weights = self.encoder(input_ids, segment_ids)\n",
        "\n",
        "        # 문장 수준 표현을 위한 [CLS] 토큰 벡터 추출 및 변환\n",
        "        cls_output = encoder_output[:, 0].contiguous()\n",
        "        cls_output = self.linear_cls(cls_output)\n",
        "        cls_output = self.activation_cls(cls_output)\n",
        "\n",
        "        return encoder_output, cls_output, attention_weights\n",
        "\n",
        "    def save(self, epoch, loss, path):\n",
        "        \"\"\"\n",
        "        모델 저장 함수\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            \"state_dict\": self.state_dict()\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\"\n",
        "        저장된 모델 로드 함수\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]"
      ],
      "metadata": {
        "id": "VPgXnIrg0vcD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. BERT Pretrain class 개요 및 구현\n",
        "BERT는 Transformer Encoder를 기반으로 한 비생성형 사전학습 모델입니다.\n",
        "Transformer의 생성형 학습 방식과는 달리, BERT는 양방향 문맥 이해에 특화된 사전학습 목표를 사용합니다.\n",
        "\n",
        "### 사전학습 목표\n",
        "- Masked Language Modeling (MLM):   \n",
        "  입력 문장에서 일부 토큰을 [MASK]로 가리고 해당 마스크된 위치의 원래 단어를 예측하는 과제\n",
        "- Next Sentence Prediction (NSP):   \n",
        "  두 문장이 주어졌을 때 두 번째 문장이 실제 다음 문장인지 이진 분류하는 과제\n",
        "\n",
        "### 구현 개요\n",
        "- BERT 인코더의 출력(Contextual Embeddings)을 바탕으로   \n",
        "MLM과 NSP를 위한 두 개의 출력 레이어를 구성합니다.\n",
        "- 이 출력값들을 통해 각각의 손실(loss) 을 계산하고,   \n",
        "두 손실을 합산하여 모델을 학습시켰습니다."
      ],
      "metadata": {
        "id": "dFnvb2ae2lrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# BERT 사전학습을 위한 모델 클래스 정의\n",
        "class BERTPretrain(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # BERT 인코더 모델 불러오기\n",
        "        self.bert = BERTModel(self.config)\n",
        "\n",
        "        # NSP (다음 문장 예측)를 위한 이진 분류기 정의\n",
        "        self.projection_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n",
        "\n",
        "        # MLM (마스크된 단어 예측)를 위한 출력 계층 정의\n",
        "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
        "\n",
        "        # 입력 임베딩 레이어와 출력 계층의 가중치 공유 (Weight Tying)\n",
        "        # self.bert.encoder 내부의 token_embedding을 참조\n",
        "        self.projection_lm.weight = self.bert.encoder.token_embedding.weight\n",
        "\n",
        "    def forward(self, token_ids, segment_ids):\n",
        "        # BERT 인코더를 통과하여 전체 문맥 벡터, [CLS] 벡터, 어텐션 확률을 가져옴\n",
        "        sequence_output, cls_output, attn_weights = self.bert(token_ids, segment_ids)\n",
        "\n",
        "        # NSP 예측: [CLS] 토큰 벡터를 분류기에 입력하여 두 문장이 연속인지 판단\n",
        "        logits_cls = self.projection_cls(cls_output)\n",
        "\n",
        "        # MLM 예측: 모든 토큰 위치에서 원래 단어를 예측\n",
        "        logits_lm = self.projection_lm(sequence_output)\n",
        "\n",
        "        # 각 태스크에 대한 출력값 반환\n",
        "        return logits_cls, logits_lm, attn_weights"
      ],
      "metadata": {
        "id": "9xWVKMTl4RN1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.1 MLM을 위한 마스크 생성 함수"
      ],
      "metadata": {
        "id": "yMQIxBkb7RuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# 마스킹된 토큰을 생성하는 함수\n",
        "def generate_masked_tokens(token_list, mask_count, vocab_tokens):\n",
        "    candidate_indices = []\n",
        "    for idx, token in enumerate(token_list):\n",
        "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
        "            continue\n",
        "        if candidate_indices and not token.startswith(u\"\\u2581\"):\n",
        "            candidate_indices[-1].append(idx)\n",
        "        else:\n",
        "            candidate_indices.append([idx])\n",
        "    random.shuffle(candidate_indices)\n",
        "\n",
        "    masked_token_info = []\n",
        "    for indices in candidate_indices:\n",
        "        if len(masked_token_info) >= mask_count:\n",
        "            break\n",
        "        if len(masked_token_info) + len(indices) > mask_count:\n",
        "            continue\n",
        "        for index in indices:\n",
        "            original_token = token_list[index]\n",
        "            if random.random() < 0.8:\n",
        "                token_list[index] = \"[MASK]\"\n",
        "            elif random.random() < 0.5:\n",
        "                token_list[index] = original_token\n",
        "            else:\n",
        "                token_list[index] = random.choice(vocab_tokens)\n",
        "            masked_token_info.append({\"index\": index, \"label\": original_token})\n",
        "\n",
        "    masked_token_info.sort(key=lambda x: x[\"index\"])\n",
        "    mask_indices = [item[\"index\"] for item in masked_token_info]\n",
        "    mask_labels = [item[\"label\"] for item in masked_token_info]\n",
        "\n",
        "    return token_list, mask_indices, mask_labels"
      ],
      "metadata": {
        "id": "QrB3DNz77cTl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.2 NSP용 인스턴스 및 전체 pretrain 데이터 생성 함수"
      ],
      "metadata": {
        "id": "TKu0rL-l7b7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from random import randrange, random\n",
        "\n",
        "# 두 문장의 길이를 맞추기 위해 자르는 함수\n",
        "def trim_sequences(seq_a, seq_b, max_total_length):\n",
        "    while len(seq_a) + len(seq_b) > max_total_length:\n",
        "        if len(seq_a) > len(seq_b):\n",
        "            seq_a.pop(0)\n",
        "        else:\n",
        "            seq_b.pop()\n",
        "\n",
        "# BERT 사전학습용 인스턴스를 생성하는 함수\n",
        "def generate_pretrain_instances(all_documents, doc_idx, current_doc, max_length, mask_ratio, vocab_tokens):\n",
        "    max_tokens = max_length - 3  # [CLS], [SEP], [SEP]\n",
        "    target_length = max_tokens\n",
        "\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    chunk_length = 0\n",
        "    for i in range(len(current_doc)):\n",
        "        current_chunk.append(current_doc[i])\n",
        "        chunk_length += len(current_doc[i])\n",
        "        if i == len(current_doc) - 1 or chunk_length >= target_length:\n",
        "            if current_chunk:\n",
        "                split_idx = 1\n",
        "                if len(current_chunk) > 1:\n",
        "                    split_idx = randrange(1, len(current_chunk))\n",
        "                tokens_a = []\n",
        "                for j in range(split_idx):\n",
        "                    tokens_a.extend(current_chunk[j])\n",
        "\n",
        "                tokens_b = []\n",
        "                if len(current_chunk) == 1 or random() < 0.5:\n",
        "                    is_next = 0\n",
        "                    random_idx = doc_idx\n",
        "                    while random_idx == doc_idx:\n",
        "                        random_idx = randrange(0, len(all_documents))\n",
        "                    random_doc = all_documents[random_idx]\n",
        "                    random_start = randrange(0, len(random_doc))\n",
        "                    for j in range(random_start, len(random_doc)):\n",
        "                        tokens_b.extend(random_doc[j])\n",
        "                else:\n",
        "                    is_next = 1\n",
        "                    for j in range(split_idx, len(current_chunk)):\n",
        "                        tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                trim_sequences(tokens_a, tokens_b, max_tokens)\n",
        "\n",
        "                assert tokens_a\n",
        "                assert tokens_b\n",
        "\n",
        "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "                segments = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "                tokens, mask_indices, mask_labels = generate_masked_tokens(tokens, int((len(tokens) - 3) * mask_ratio), vocab_tokens)\n",
        "\n",
        "                instance = {\n",
        "                    \"tokens\": tokens,\n",
        "                    \"segment\": segments,\n",
        "                    \"is_next\": is_next,\n",
        "                    \"mask_idx\": mask_indices,\n",
        "                    \"mask_label\": mask_labels\n",
        "                }\n",
        "                instances.append(instance)\n",
        "            current_chunk = []\n",
        "            chunk_length = 0\n",
        "    return instances"
      ],
      "metadata": {
        "id": "gNs8iNKZ7xQv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.3 Pretrain 데이터 생성"
      ],
      "metadata": {
        "id": "PLhNlwLeCsug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "import json, os, random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# vocab 불러오기\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(\"/content/drive/MyDrive/bert/mini_emotion_tokenizer_7k.model\")\n",
        "\n",
        "# 감정 데이터셋\n",
        "ds = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
        "docs = [[vocab.encode_as_pieces(row[\"text\"])] for row in ds]\n",
        "\n",
        "# 예시용 generate 함수\n",
        "def generate_pretrain_instances(docs, doc_idx, doc, seq_length, mask_ratio, vocab_tokens):\n",
        "    # 간단한 dummy 예시\n",
        "    tokens = sum(doc, [])[:seq_length-3]\n",
        "    return [{\n",
        "        \"tokens\": [\"[CLS]\"] + tokens + [\"[SEP]\"],\n",
        "        \"segment\": [0] * (len(tokens) + 2),\n",
        "        \"is_next\": 1,\n",
        "        \"mask_idx\": [],\n",
        "        \"mask_label\": []\n",
        "    }]\n",
        "\n",
        "# Pretrain 데이터 생성 함수\n",
        "def build_pretrain_dataset_from_docs(docs, vocab, output_path_template, num_files, seq_length, mask_ratio):\n",
        "    vocab_tokens = [vocab.id_to_piece(i) for i in range(vocab.get_piece_size()) if not vocab.is_unknown(i)]\n",
        "    for i in range(num_files):\n",
        "        output_file = output_path_template.format(i)\n",
        "        if os.path.exists(output_file):\n",
        "            print(f\"[경고] {output_file} 파일이 이미 존재하여 덮어씁니다.\")\n",
        "            os.remove(output_file)\n",
        "\n",
        "        total_written = 0\n",
        "        with open(output_file, \"w\") as out_f:\n",
        "            for doc_idx, doc in enumerate(tqdm(docs, desc=f\"Creating File {i}\")):\n",
        "                inst_list = generate_pretrain_instances(docs, doc_idx, doc, seq_length, mask_ratio, vocab_tokens)\n",
        "                for inst in inst_list:\n",
        "                    out_f.write(json.dumps(inst) + \"\\n\")\n",
        "                    total_written += 1\n",
        "        print(f\"[완료] {output_file}: {total_written}개 instance 저장 완료\")\n",
        "\n",
        "# 실행\n",
        "build_pretrain_dataset_from_docs(\n",
        "    docs,\n",
        "    vocab,\n",
        "    output_path_template=\"/content/drive/MyDrive/bert/emotion_bert_pretrain_{:02d}.json\",\n",
        "    num_files=1,\n",
        "    seq_length=256,\n",
        "    mask_ratio=0.15\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbw6zwIfHoxC",
        "outputId": "af214a90-d0c3-4d1c-fc66-34df20604151"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[경고] /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json 파일이 이미 존재하여 덮어씁니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating File 0: 100%|██████████| 16000/16000 [00:00<00:00, 17002.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[완료] /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 16000개 instance 저장 완료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터가 잘 생성되었는지 확인해봅시다."
      ],
      "metadata": {
        "id": "bESqSySCEtfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validate_pretrain_dataset.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# 출력 파일 경로\n",
        "output_file = \"/content/drive/MyDrive/bert/emotion_bert_pretrain_00.json\"\n",
        "\n",
        "# 1. 파일 존재 여부 확인\n",
        "file_exists = os.path.exists(output_file)\n",
        "\n",
        "# 2. 파일 내용 확인\n",
        "if file_exists:\n",
        "    with open(output_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        line_count = len(lines)\n",
        "        # 첫 번째 인스턴스 로드\n",
        "        first_instance = json.loads(lines[0]) if line_count > 0 else {}\n",
        "\n",
        "        # 필수 키 확인\n",
        "        required_keys = {\"tokens\", \"segment\", \"is_next\", \"mask_idx\", \"mask_label\"}\n",
        "        missing_keys = [key for key in required_keys if key not in first_instance]\n",
        "        all_keys_present = len(missing_keys) == 0\n",
        "else:\n",
        "    line_count = 0\n",
        "    first_instance = {}\n",
        "    missing_keys = list({\"tokens\", \"segment\", \"is_next\", \"mask_idx\", \"mask_label\"})\n",
        "    all_keys_present = False\n",
        "\n",
        "# 결과 출력\n",
        "print(\"파일 존재 여부:\", file_exists)\n",
        "print(\"인스턴스 개수:\", line_count)\n",
        "print(\"첫 번째 인스턴스:\", first_instance)\n",
        "print(\"누락된 키:\", missing_keys)\n",
        "print(\"모든 키 존재 여부:\", all_keys_present)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmhI9aAGEywZ",
        "outputId": "ee6d554f-3d4a-4180-bf04-51ef9069963e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일 존재 여부: True\n",
            "인스턴스 개수: 16000\n",
            "첫 번째 인스턴스: {'tokens': ['[CLS]', '▁i', '▁didnt', '▁feel', '▁humiliated', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0], 'is_next': 1, 'mask_idx': [], 'mask_label': []}\n",
            "누락된 키: []\n",
            "모든 키 존재 여부: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.4 감정 분석 BERT Pretrain Dataset 구성\n",
        "\n",
        "#### 7.4.1 EmotionPretrainDataset 클래스(MLM을 위한 라벨 구성)\n",
        "\n",
        "이 클래스는 감정 분석용 BERT 모델을 사전학습(pretrain)하기 위한 데이터셋을 정의합니다.\n",
        "\n",
        "입력 파일(emotion_bert_pretrain_00.json)을 한 줄씩 읽으며 다음과 같은 정보를 수집합니다.\n",
        "- is_next: 두 문장이 실제로 이어지는 문장인지 판단하는 NSP(Next Sentence Prediction) 라벨입니다.\n",
        "- tokens: 문장을 SentencePiece로 분할한 토큰 리스트입니다.\n",
        "- segment: 첫 번째 문장은 0, 두 번째 문장은 1로 구분한 세그먼트 정보입니다.\n",
        "- mask_idx: 마스킹된 토큰의 위치를 나타냅니다.\n",
        "- mask_label: 마스킹된 토큰이 원래 어떤 단어였는지를 나타내는 정답입니다.\n",
        "\n",
        "이 정보를 바탕으로 다음을 수행합니다:\n",
        "\n",
        "1. 문장 토큰들을 vocab의 ID로 변환하여 sentences에 저장합니다.\n",
        "2. 세그먼트 정보를 segments에 저장합니다.\n",
        "3. label_lm이라는 시퀀스를 생성하고, 모든 값은 기본적으로 -1로 채워집니다.\n",
        "4. 마스킹된 위치(mask_idx)에만 정답(mask_label)을 넣습니다."
      ],
      "metadata": {
        "id": "NAzFM2rPNFb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionPretrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, input_json_path):\n",
        "        self.vocab = vocab\n",
        "        self.labels_cls = []    # NSP (Next Sentence Prediction) 라벨 저장\n",
        "        self.labels_lm = []     # MLM (Masked Language Model) 라벨 저장\n",
        "        self.sentences = []     # 토큰 ID로 변환된 문장 시퀀스\n",
        "        self.segments = []      # 세그먼트 정보 (문장 A=0, 문장 B=1)\n",
        "\n",
        "        # 전체 라인 수 계산\n",
        "        with open(input_json_path, \"r\") as f:\n",
        "            line_count = sum(1 for _ in f)\n",
        "\n",
        "        # JSONL 파일을 한 줄씩 읽으며 처리\n",
        "        with open(input_json_path, \"r\") as f:\n",
        "            for line in tqdm(f, total=line_count, desc=f\"로딩 중: {input_json_path}\", unit=\"줄\"):\n",
        "                instance = json.loads(line)\n",
        "\n",
        "                # NSP 라벨\n",
        "                self.labels_cls.append(instance[\"is_next\"])\n",
        "\n",
        "                # 문장을 토큰 ID로 변환\n",
        "                token_ids = [vocab.piece_to_id(p) for p in instance[\"tokens\"]]\n",
        "                self.sentences.append(token_ids)\n",
        "\n",
        "                # 세그먼트 정보\n",
        "                self.segments.append(instance[\"segment\"])\n",
        "\n",
        "                # MLM 라벨 처리\n",
        "                mask_indices = np.array(instance[\"mask_idx\"], dtype=np.int32)\n",
        "                mask_labels = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int32)\n",
        "\n",
        "                # 라벨 시퀀스를 -1로 초기화한 후 마스킹 위치만 실제 정답으로 대체\n",
        "                lm_labels = np.full(len(token_ids), fill_value=-1, dtype=np.int32)\n",
        "                lm_labels[mask_indices] = mask_labels\n",
        "                self.labels_lm.append(lm_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        # 모든 항목의 길이가 같아야 함\n",
        "        assert len(self.labels_cls) == len(self.labels_lm) == len(self.sentences) == len(self.segments)\n",
        "        return len(self.labels_cls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.labels_cls[idx]),     # NSP 라벨\n",
        "            torch.tensor(self.labels_lm[idx]),      # MLM 라벨\n",
        "            torch.tensor(self.sentences[idx]),      # 입력 문장\n",
        "            torch.tensor(self.segments[idx])        # 세그먼트 정보\n",
        "        )"
      ],
      "metadata": {
        "id": "lcNL5n-dNl2c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.4.2 Collate 함수: emotion_pretrain_collate_fn\n",
        "\n",
        "이 함수는 DataLoader에서 배치 단위로 데이터를 묶을 때 사용됩니다.   \n",
        "배치 안의 문장 길이가 다르기 때문에 각 항목에 대해 패딩 처리가 필요합니다.\n",
        "\n",
        "  - labels_lm: 정답이 없는 부분은 -1로 패딩합니다. (MLM용)\n",
        "  - inputs: 문장의 토큰 ID는 0으로 패딩합니다. (이 값은 vocab에서 pad_id로 지정된 값입니다.)\n",
        "  - segments: 문장의 구간 정보도 0으로 패딩합니다.\n",
        "\n",
        "- labels_cls: NSP용 라벨이며 길이가 1이므로 바로 stack()하여 텐서로 변환합니다."
      ],
      "metadata": {
        "id": "I6MMirRaNnKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emotion_pretrain_collate_fn(batch):\n",
        "    labels_cls, labels_lm, inputs, segments = zip(*batch)\n",
        "\n",
        "    # NSP 라벨 (0 또는 1): long 타입으로 변환\n",
        "    labels_cls = torch.stack(labels_cls, dim=0).long()\n",
        "\n",
        "    # MLM 라벨은 -1로 패딩\n",
        "    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1)\n",
        "    # 입력 시퀀스 및 세그먼트는 0으로 패딩\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
        "\n",
        "    return [\n",
        "        labels_cls,   # NSP 라벨 (batch_size,)\n",
        "        labels_lm,    # MLM 라벨 (batch_size, seq_len)\n",
        "        inputs,       # 입력 시퀀스 (batch_size, seq_len)\n",
        "        segments      # 세그먼트 정보 (batch_size, seq_len)\n",
        "    ]"
      ],
      "metadata": {
        "id": "bIbXGAZKNoru"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.4.3 DataLoader 정의\n",
        "\n",
        "위에서 만든 EmotionPretrainDataset과 collate_fn을 기반으로 DataLoader를 생성합니다.\n",
        "\n",
        "이 emotion_train_loader는 BERT 사전학습(pretraining)에서 사용될 수 있는 학습용 배치 데이터를 제공합니다."
      ],
      "metadata": {
        "id": "104Xrkm5NqLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "json_path = f\"{data_dir}/emotion_bert_pretrain_00.json\"\n",
        "emotion_dataset = EmotionPretrainDataset(vocab, json_path)\n",
        "emotion_train_loader = torch.utils.data.DataLoader(\n",
        "    emotion_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=emotion_pretrain_collate_fn\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taTehjaCNqb_",
        "outputId": "8574ed0d-f9f6-4b28-a238-27934387c422"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "로딩 중: /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 100%|██████████| 16000/16000 [00:01<00:00, 8308.06줄/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Pretrain\n",
        "\n",
        "#### 8.1 한 epoch 학습을 수행하는 함수\n",
        "이 함수는 BERT Pretrain을 위한 학습 루틴 중, 한 epoch 동안의 모델 학습을 수행하는 함수입니다.   \n",
        "NSP와 MLM 두 가지 목적을 함께 학습합니다.   \n",
        "사전학습 데이터가 준비된 상태에서 Epoch 단위로 반복 실행하며, 모델이 문맥을 이해하고 마스킹된 단어를 예측합니다."
      ],
      "metadata": {
        "id": "fJsbkBaWOIe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 epoch 동안 BERT Pretrain 모델 학습 수행\n",
        "def train_one_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, data_loader):\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    loss_log = []  # MLM loss 누적 기록용 리스트\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    with tqdm(total=len(data_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            # 배치 데이터를 GPU 또는 CPU로 이동\n",
        "            label_cls, label_lm, token_ids, segment_ids = map(lambda x: x.to(config.device), batch)\n",
        "\n",
        "            # CrossEntropyLoss는 long 타입 필요 → 명시적으로 타입 캐스팅\n",
        "            label_cls = label_cls.long()\n",
        "            label_lm = label_lm.long()\n",
        "\n",
        "            # 옵티마이저의 기울기 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 모델 순전파: NSP, MLM, Attention 출력을 반환\n",
        "            logits_cls, logits_lm, _ = model(token_ids, segment_ids)\n",
        "\n",
        "            # NSP Loss 계산 (Binary Classification)\n",
        "            loss_cls = criterion_cls(logits_cls, label_cls)\n",
        "\n",
        "            # MLM Loss 계산 (다중 클래스 분류, ignore_index=-1)\n",
        "            loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(-1)), label_lm.view(-1))\n",
        "\n",
        "            # 두 개의 Loss를 더해 최종 손실 계산\n",
        "            total_loss = loss_cls + loss_lm\n",
        "\n",
        "            # MLM loss만 로깅에 사용\n",
        "            loss_log.append(loss_lm.item())\n",
        "\n",
        "            # 역전파 및 가중치 업데이트\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 진행 상태 표시\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss_lm.item():.3f} (Avg: {np.mean(loss_log):.3f})\")\n",
        "\n",
        "    # MLM 평균 손실 반환\n",
        "    return np.mean(loss_log)"
      ],
      "metadata": {
        "id": "E0iao6jLO11h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8.2 전체 학습 실행 코드\n",
        "아래 구현한 코드는 BERT Pretrain을 여러 epoch에 걸쳐 반복 실행하는 전체 학습 루틴을 정의합니다.   \n",
        "모델 초기화부터 학습 손실 계산, 모델 저장까지의 모든 과정을 포함합니다."
      ],
      "metadata": {
        "id": "OetFtb7NO2vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 디바이스 설정 (GPU가 가능하면 CUDA 사용)\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(config)\n",
        "\n",
        "# 학습 관련 설정\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 20\n",
        "\n",
        "# BERT Pretrain 모델 정의 및 저장 경로\n",
        "model = BERTPretrain(config)\n",
        "pretrain_model_path = f\"{data_dir}/save_bert_pretrain.pth\"\n",
        "\n",
        "# 기존 저장 모델이 있다면 로드\n",
        "start_epoch, best_loss = 0, 0\n",
        "if os.path.isfile(pretrain_model_path):\n",
        "    start_epoch, best_loss = model.bert.load(pretrain_model_path)\n",
        "    print(f\"[불러오기 완료] epoch={start_epoch}, loss={best_loss:.4f}\")\n",
        "    start_epoch += 1  # 이어서 학습할 수 있도록 epoch 증가\n",
        "\n",
        "# 모델을 디바이스에 올리기\n",
        "model.to(config.device)\n",
        "\n",
        "# Loss 함수 정의\n",
        "criterion_mlm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction=\"mean\")  # MLM은 -1 패딩 무시\n",
        "criterion_nsp = torch.nn.CrossEntropyLoss()  # NSP용 loss\n",
        "\n",
        "# 옵티마이저 정의 (Adam 사용)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 학습 루프\n",
        "train_losses = []\n",
        "for epoch_offset in range(num_epochs):\n",
        "    epoch = start_epoch + epoch_offset\n",
        "\n",
        "    # (선택) 여러 개의 json 파일로 데이터를 분할한 경우, epoch에 따라 다른 파일을 불러올 수도 있음\n",
        "    if epoch_offset > 0:\n",
        "        del emotion_train_loader  # 이전 DataLoader 제거\n",
        "        emotion_dataset = EmotionPretrainDataset(vocab, f\"{data_dir}/emotion_bert_pretrain_{epoch % 1:02d}.json\")\n",
        "        emotion_train_loader = torch.utils.data.DataLoader(\n",
        "            emotion_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=emotion_pretrain_collate_fn\n",
        "        )\n",
        "\n",
        "    # 1 epoch 학습 수행\n",
        "    avg_loss = train_one_epoch(\n",
        "        config, epoch, model,\n",
        "        criterion_mlm, criterion_nsp,\n",
        "        optimizer, emotion_train_loader\n",
        "    )\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # 모델 저장\n",
        "    model.bert.save(epoch, avg_loss, pretrain_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5L72uFO2Za",
        "outputId": "78956935-697d-438e-e58b-2d5ace8cdbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_enc_vocab': 7000, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cpu')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train(0): 100%|██████████| 125/125 [19:20<00:00,  9.28s/it, Loss: nan (Avg: nan)]\n",
            "로딩 중: /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 100%|██████████| 16000/16000 [00:00<00:00, 29382.91줄/s]\n",
            "Train(1): 100%|██████████| 125/125 [18:24<00:00,  8.84s/it, Loss: nan (Avg: nan)]\n",
            "로딩 중: /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 100%|██████████| 16000/16000 [00:00<00:00, 28696.27줄/s]\n",
            "Train(2): 100%|██████████| 125/125 [18:29<00:00,  8.87s/it, Loss: nan (Avg: nan)]\n",
            "로딩 중: /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 100%|██████████| 16000/16000 [00:01<00:00, 8624.70줄/s] \n",
            "Train(3): 100%|██████████| 125/125 [18:26<00:00,  8.85s/it, Loss: nan (Avg: nan)]\n",
            "로딩 중: /content/drive/MyDrive/bert/emotion_bert_pretrain_00.json: 100%|██████████| 16000/16000 [00:00<00:00, 24889.49줄/s]\n",
            "Train(4):  58%|█████▊    | 73/125 [11:21<08:51, 10.21s/it, Loss: nan (Avg: nan)]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Pretrain 손실 시각화 및 확인\n",
        "이제 마지막으로 아래 코드를 통해 BERT 모델을 pretrain하면서 매 epoch마다 수집된 손실 값을 표와 그래프로 확인해보며 BERT 모델 구현 및 생성을 완료합니다."
      ],
      "metadata": {
        "id": "SkSzmb0LmRdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 손실 값을 DataFrame으로 정리\n",
        "df = pd.DataFrame({\n",
        "    \"Epoch\": list(range(1, len(train_losses) + 1)),\n",
        "    \"Loss\": train_losses\n",
        "})\n",
        "\n",
        "# 표 형태로 손실 확인\n",
        "display(df)\n",
        "\n",
        "# 손실 추이 시각화\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df[\"Epoch\"], df[\"Loss\"], marker='o', linestyle='-', color='royalblue', label=\"MLM Loss\")\n",
        "plt.title(\"BERT Pretrain: Epoch vs. Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.xticks(df[\"Epoch\"])\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TSjFNgdfVW2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}