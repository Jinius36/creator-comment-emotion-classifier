{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 환경 설정 및 라이브러리 설치\n",
        "\n"
      ],
      "metadata": {
        "id": "VuYcvNLhN5PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate evaluate\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_metric\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "AjVMLNaaN3OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 GPU 설정"
      ],
      "metadata": {
        "id": "wkz08J2EOeXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 장치: {device}\")"
      ],
      "metadata": {
        "id": "LbktYifLOayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 구글 드라이브 마운트"
      ],
      "metadata": {
        "id": "h6yPFQHoOh1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nDqTdlMGOkK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 하이브리드 토크나이저 로드"
      ],
      "metadata": {
        "id": "UDsYu1T8Onvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_FILE = \"/content/drive/MyDrive/bert/hybrid_bert_vocab.txt\"\n",
        "\n",
        "if not os.path.exists(VOCAB_FILE):\n",
        "    raise FileNotFoundError(f\" 오류: {VOCAB_FILE} 경로에 보캡 파일이 없습니다. 파일을 업로드하거나 경로를 수정하세요.\")\n",
        "\n",
        "# .txt 파일로 BertTokenizer 생성\n",
        "tokenizer = BertTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True)\n",
        "print(f\" 하이브리드 Vocab 로드 완료! 크기: {len(tokenizer)} (15,477개 예상)\")\n"
      ],
      "metadata": {
        "id": "TLLg23oJOy_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 데이터셋 로드 및 전처리\n"
      ],
      "metadata": {
        "id": "e2ZqJcJoOz1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# 토큰화 수행\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "# 불필요한 컬럼 제거 및 PyTorch 포맷 설정\n",
        "columns_to_keep = ['input_ids', 'attention_mask', 'label']\n",
        "columns_to_remove = [col for col in tokenized_datasets['train'].column_names if col not in columns_to_keep]\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\" 데이터 전처리 완료.\")"
      ],
      "metadata": {
        "id": "nEjXS6bkO4zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 모델 로드 및 임베딩 사이즈 조정"
      ],
      "metadata": {
        "id": "66u9d_ZvO-OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "num_labels = 6\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "\n",
        "print(f\" 임베딩 사이즈 조정: {model.config.vocab_size} -> {len(tokenizer)}\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "XJgGxGoOO-6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 학습 설정 및 시작 (Trainer)"
      ],
      "metadata": {
        "id": "pXQnvYe4PCQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 학습 파라미터 (표준 BERT에 최적화됨)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_hybrid_finetuning\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,                      # GPU 가속 및 메모리 절약\n",
        "    gradient_accumulation_steps=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "JpkMk7NcPE5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}